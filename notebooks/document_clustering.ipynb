{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Document Clustering with LayoutLMv2\r\n",
        "\r\n",
        "This notebook provides code for document classification on Cerberus documents from a sample data room.\r\n",
        "\r\n",
        "The pipeline has been tested on an Azure ML notebook with a `Python 3` kernel, using a `STANDARD_NC6S_V3` as the compute instance.\r\n",
        "\r\n",
        "Based on document type, documents are clustered in different ways. PDF/DOC(X), and RTF formats will undergo the recommended modeling process (LayoutLMv2, KMeans, KeyBERT). The remaining document types will be clustered and labeled based on format type.\r\n",
        "\r\n",
        "<p align=\"center\"><img width=70% src=\"../imgs/workflow_detailed.png\"></p>\r\n",
        "\r\n",
        "This version provides batch processing of documents which significantly increases the efficiency of the model compared with the previous version. A benchmark is shown in the following figure.\r\n",
        "\r\n",
        "<p align=\"center\"><img width=70% src=\"../imgs/100_docs_benchmark.png\"></p>\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\r\n",
        "The document clustering model is composed of three models, 1) LayoutLMv2 document feature extraction model, 2) KeyBERT keyword extraction model, and 3) KMeans clustering model. The required dependencies can be installed by running `./install.sh` in the terminal. One can open a terminal window by clicking the terminal icon on the left of the notebook toolbar on the top."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "\r\n",
        "# general functions\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import glob\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import logging\r\n",
        "import yaml\r\n",
        "import argparse\r\n",
        "import shutil\r\n",
        "from ast import literal_eval\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from collections import defaultdict\r\n",
        "import multiprocessing\r\n",
        "from multiprocessing import Pool \r\n",
        "\r\n",
        "# modeling functions\r\n",
        "import torch\r\n",
        "from pdf2image import convert_from_path\r\n",
        "\r\n",
        "from transformers import (LayoutLMv2FeatureExtractor, \r\n",
        "    LayoutLMv2Tokenizer, \r\n",
        "    LayoutLMv2Processor, \r\n",
        "    LayoutLMv2ForSequenceClassification\r\n",
        ")\r\n",
        "\r\n",
        "from transformers import logging as transformer_logger\r\n",
        "transformer_logger.set_verbosity_error()\r\n",
        "os.environ[\"SFL_SCIENTIFIC_CODE_REPO\"] = os.path.dirname(os.getcwd())\r\n",
        "PROJECT_ROOT = os.environ[\"SFL_SCIENTIFIC_CODE_REPO\"]\r\n",
        "sys.path.insert(0, PROJECT_ROOT)\r\n",
        "\r\n",
        "from src.model import (assign_discriptions,\r\n",
        "                   batch_inference,\r\n",
        "                   cluster_kmeans_elbow,\r\n",
        "                   pdf2keywords_keybert,    \r\n",
        "                   process_png            \r\n",
        ")\r\n",
        "\r\n",
        "from src.preprocess import (doc_to_pdf_multi,\r\n",
        "                   get_type_path_dict,\r\n",
        "                   get_failed_pdf,\r\n",
        "                   multi_process_pdf_to_img               \r\n",
        ")\r\n",
        "\r\n",
        "from src.utils import make_tmp_dir\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1647364281690
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Model Inputs\r\n",
        "A config file `notebook_config.yaml` contains all input parameters for the model, such as the path to the root of the data room `data_room_root_path` and the user-defined number of clusters `user_define_k`.\r\n",
        "\r\n",
        "For further details on how to choose the number of clusters, one can refer to the clustering section in this notebook. All other default hyperparameters are specified in the config file. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# general settings\r\n",
        "pd.set_option('max_colwidth', -1)\r\n",
        "np.random.seed(3)\r\n",
        "TIMESTR = time.strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "CONFIGPATH = \"../config/doc_cls_config.yaml\"\r\n",
        "LOGPATH = f\"{TIMESTR}.log\"\r\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\n",
        "# open config file and load\r\n",
        "with open(CONFIGPATH, \"r\") as f:\r\n",
        "    config = yaml.load(f,Loader=yaml.Loader)\r\n",
        "\r\n",
        "# load config info\r\n",
        "DATAROOM_ROOT_PATH = config['inputs']['data_room_root_path']\r\n",
        "DOC_TMP_FOLDER = config['storage']['unprocessed_doc_dir']\r\n",
        "TRANSFROMED_PDF_FOLDER =config['storage']['processed_dir']\r\n",
        "TOP_N_KEYWORDS = config['keyword_extraction']['top_n_keywords']\r\n",
        "KEYPHRASE_NGRAM_RANGE = config['keyword_extraction']['keyphrase_ngram_range']\r\n",
        "ENCODING_LIMIT = config['layoutlm_model']['encoding_limit']\r\n",
        "BATCH_SIZE = config['layoutlm_model']['batch_size']\r\n",
        "FIXK = config['kmeans_model']['user_define_k']\r\n",
        "MAX_TRIES = config['kmeans_model']['max_tries']\r\n",
        "NUM_PROCESSOR = config['multi_process']['num_processor']\r\n",
        "OUTPUT_PATH = config['outputs']['json_path']\r\n",
        "\r\n",
        "# remove intermediate folders from previous runs\r\n",
        "if os.path.exists(DOC_TMP_FOLDER):\r\n",
        "    shutil.rmtree(DOC_TMP_FOLDER)\r\n",
        "\r\n",
        "if os.path.exists(TRANSFROMED_PDF_FOLDER):\r\n",
        "    shutil.rmtree(TRANSFROMED_PDF_FOLDER)\r\n",
        "\r\n",
        "os.makedirs(TRANSFROMED_PDF_FOLDER)\r\n",
        "logging.basicConfig(filename=LOGPATH,level=logging.INFO,format= '[%(asctime)s] %(levelname)s - %(message)s',datefmt='%H:%M:%S')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647364282365
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Doc(x) and RTF Files\r\n",
        "Doc(x) and RTF files will be converted to pdf format for further processing."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get document type information\r\n",
        "doc_type_dict = get_type_path_dict(DATAROOM_ROOT_PATH)\r\n",
        "existing_doc_types = list(doc_type_dict.keys())\r\n",
        "\r\n",
        "doc_docx_rtf_path_list=[]\r\n",
        "for doc_type in ['doc','docx','rtf']:\r\n",
        "    if doc_type in existing_doc_types:\r\n",
        "        doc_docx_rtf_path_list.extend(doc_type_dict[f\"{doc_type}\"])\r\n",
        "\r\n",
        "pdf_path_list=[]\r\n",
        "for doc_type in ['pdf']:\r\n",
        "    if doc_type in existing_doc_types:\r\n",
        "        pdf_path_list.extend(doc_type_dict[f\"{doc_type}\"])\r\n",
        "\r\n",
        "png_path_list=[]\r\n",
        "for doc_type in ['png']:\r\n",
        "    if doc_type in existing_doc_types:\r\n",
        "        png_path_list.extend(doc_type_dict[f\"{doc_type}\"])\r\n",
        "\r\n",
        "# Process doc(x)/rtf files in dataroom\r\n",
        "if not len(doc_docx_rtf_path_list)==0:\r\n",
        "\r\n",
        "    st = time.time()\r\n",
        "    # Save docs, doc, rtf to seperate temporary folders and keep track of them\r\n",
        "    new_doc_path_list = make_tmp_dir(doc_docx_rtf_path_list, DOC_TMP_FOLDER)\r\n",
        "\r\n",
        "    # Libreoffice has a limit of 200 documents when multiprocessing, thus process each folder with 200 one by one\r\n",
        "    for subfolder in glob.glob(os.path.join(DOC_TMP_FOLDER,\"*/\")):\r\n",
        "        if not len(glob.glob(os.path.join(subfolder,\"*.doc\")))==0:\r\n",
        "            doc_tmp_folder = os.path.join(subfolder,\"*.doc\")\r\n",
        "            doc_to_pdf_multi(doc_tmp_folder,TRANSFROMED_PDF_FOLDER)\r\n",
        "        if not len(glob.glob(os.path.join(subfolder,\"*.docx\")))==0:\r\n",
        "            doc_tmp_folder = os.path.join(subfolder,\"*.docx\")\r\n",
        "            doc_to_pdf_multi(doc_tmp_folder,TRANSFROMED_PDF_FOLDER)\r\n",
        "        if not len(glob.glob(os.path.join(subfolder,\"*.rtf\")))==0:\r\n",
        "            doc_tmp_folder = os.path.join(subfolder,\"*.rtf\")\r\n",
        "            doc_to_pdf_multi(doc_tmp_folder,TRANSFROMED_PDF_FOLDER)\r\n",
        "    \r\n",
        "    end = time.time()\r\n",
        "    logging.info(f\"doc(x)/rtf to pdf format, spent: {end-st}s\")\r\n",
        "\r\n",
        "    # Get transformed pdf path list for further processing\r\n",
        "    transformed_pdf_file_list = glob.glob(os.path.join(TRANSFROMED_PDF_FOLDER , f\"**/*.pdf\"), recursive=True)\r\n",
        "\r\n",
        "    # Format dataframe for storing results for doc, docx, rtf\r\n",
        "    final_output = pd.DataFrame(doc_docx_rtf_path_list, columns =['document_path'], dtype = str)\r\n",
        "    final_output[\"corrupt\"]=0\r\n",
        "\r\n",
        "    # Keep track of corrupted docs\r\n",
        "    failed_document_id = get_failed_pdf(new_doc_path_list, transformed_pdf_file_list)\r\n",
        "    final_output.iloc[failed_document_id, final_output.columns.get_loc('corrupt')] = 1\r\n",
        "\r\n",
        "    tmp_doc_pdf_list = []\r\n",
        "    pdf_id=0\r\n",
        "    for i in range(len(doc_docx_rtf_path_list)):\r\n",
        "        if i in failed_document_id:\r\n",
        "            tmp_doc_pdf_list.append(\"None\")\r\n",
        "        else:\r\n",
        "            tmp_doc_pdf_list.append(transformed_pdf_file_list[pdf_id])\r\n",
        "            pdf_id+=1\r\n",
        "\r\n",
        "    final_output[\"pdf_path\"]=tmp_doc_pdf_list\r\n",
        "\r\n",
        "else:\r\n",
        "    # Create empty dataframe\r\n",
        "    final_output= pd.DataFrame([], columns =['document_path','pdf_path', 'corrupt'], dtype = str)\r\n",
        "\r\n",
        "\r\n",
        "if not len(pdf_path_list)==0:\r\n",
        "    # Append original pdf list to existing dataframe\r\n",
        "    pdf_output = pd.DataFrame(pdf_path_list, columns =['document_path'], dtype = str)\r\n",
        "    pdf_output['corrupt']=0\r\n",
        "    pdf_output[\"pdf_path\"]=pdf_path_list\r\n",
        "    final_output = pd.concat([final_output, pdf_output], axis=0).reset_index(drop=True)\r\n",
        "\r\n",
        "    df_uncorrupted_documents = final_output[final_output['corrupt']==0]\r\n",
        "    df_corrupted_documents = final_output[final_output['corrupt']==1]\r\n",
        "    final_output = pd.concat([df_uncorrupted_documents, df_corrupted_documents], axis=0).reset_index(drop=True)\r\n",
        "\r\n",
        "else:\r\n",
        "    # Create empty dataframe\r\n",
        "    df_uncorrupted_documents = final_output[final_output['corrupt']==0]\r\n",
        "    df_corrupted_documents = final_output[final_output['corrupt']==1]\r\n",
        "    final_output = pd.concat([df_uncorrupted_documents, df_corrupted_documents], axis=0).reset_index(drop=True)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647364284540
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process PDF Files\r\n",
        "\r\n",
        "PDFs in the data room and the DOC(X) and RTF files that are converted to PDFs, will be further converted to images for further processing."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all non corrupted pdfs\r\n",
        "if not len(final_output[final_output['corrupt']==0])==0:\r\n",
        "    st = time.time()\r\n",
        "    pdf_embeddings=[]\r\n",
        "    blank_check=[]\r\n",
        "    corrupted_pdf_id_list=[]\r\n",
        "    remaining_pdf_list = final_output[final_output[\"corrupt\"]==0][\"pdf_path\"].tolist()\r\n",
        "    pdf_to_pil_batch_sz = 100\r\n",
        "    for i in range(len(remaining_pdf_list)//pdf_to_pil_batch_sz+1):\r\n",
        "        # pdf to pil format\r\n",
        "        pdf_file_list = remaining_pdf_list[i*pdf_to_pil_batch_sz:(i+1)*pdf_to_pil_batch_sz]\r\n",
        "        image_c_list = multi_process_pdf_to_img(pdf_file_list, NUM_PROCESSOR)\r\n",
        "        corrupted_pdf_ids = [c for pil_image,c in image_c_list]\r\n",
        "        pil_image_list = [pil_image for pil_image,c in image_c_list]\r\n",
        "\r\n",
        "        #LayoutLM\r\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "        sub_pdf_embeddings, sub_blank_check = batch_inference(pil_image_list, BATCH_SIZE, device, int(ENCODING_LIMIT))\r\n",
        "        pdf_embeddings.append(sub_pdf_embeddings)\r\n",
        "        # corrupted_pdf_id_list.append(corrupted_pdf_ids)\r\n",
        "        corrupted_pdf_id_list.extend(corrupted_pdf_ids)\r\n",
        "        blank_check.extend(list(sub_blank_check))\r\n",
        "\r\n",
        "        del image_c_list\r\n",
        "        del pil_image_list\r\n",
        "\r\n",
        "        \r\n",
        "    # Keep track of corrupted pdfs\r\n",
        "    blank_pdf_check_list = [int(bc==102) for bc in blank_check]\r\n",
        "    corrupted_pdf_list = corrupted_pdf_id_list\r\n",
        "    corrupted_blank_pdf_list=[a or b for a,b in zip(corrupted_pdf_id_list,blank_pdf_check_list)]\r\n",
        "\r\n",
        "    end = time.time()\r\n",
        "    logging.info(f\"pdf to PIL format, spent: {end-st}s\")\r\n",
        "    \r\n",
        "else:\r\n",
        "    # if no pdf files in dataroom, create empty list as placeholder\r\n",
        "    corrupted_pdf_list = []\r\n",
        "    blank_pdf_check_list = []\r\n",
        "    corrupted_blank_pdf_list = []\r\n",
        "    pdf_embeddings = []\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647364369208
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process PNG Files"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not len(png_path_list)==0:\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    png_embeddings, blank_check_png_list = process_png(png_path_list,  BATCH_SIZE, device, int(ENCODING_LIMIT))\r\n",
        "    # Append png information to dataframe\r\n",
        "    png_output = pd.DataFrame(png_path_list, columns =['document_path'], dtype = str)\r\n",
        "    png_output[\"corrupt\"]=0\r\n",
        "    png_output[\"pdf_path\"]=\"image_file\"\r\n",
        "    final_output = pd.concat([final_output, png_output], axis=0).reset_index(drop=True)\r\n",
        "\r\n",
        "else:\r\n",
        "    # if no png files in dataroom, create empty list as placeholder\r\n",
        "    blank_check_png_list=[]\r\n",
        "    png_embeddings = []\r\n",
        "\r\n",
        "# Update final output with pdf and png information\r\n",
        "number_of_corrupted_doc_xdocx_rtf = len(final_output) - len(corrupted_pdf_list) - len(blank_check_png_list)\r\n",
        "final_output[\"corrupted_pdf\"] = corrupted_pdf_list + [1]*(number_of_corrupted_doc_xdocx_rtf) + blank_check_png_list\r\n",
        "final_output[\"no_content\"] = blank_pdf_check_list + [1]*(number_of_corrupted_doc_xdocx_rtf) + blank_check_png_list\r\n",
        "\r\n",
        "df_blank_documents = final_output[(final_output[\"corrupted_pdf\"]==0) & (final_output[\"no_content\"]==1)]\r\n",
        "df_corrupted_documents = final_output[(final_output[\"corrupted_pdf\"]==1)]\r\n",
        "df_uncorrupted_not_blank =  final_output[(final_output[\"no_content\"]==0) & (final_output[\"no_content\"]==0)]\r\n",
        "final_output = pd.concat([df_uncorrupted_not_blank,df_blank_documents, df_corrupted_documents], axis=0)\r\n",
        "final_output = final_output.reset_index(drop=True)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647364369435
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Clustering\r\n",
        "## Kmeans on document embeddings\r\n",
        "\r\n",
        "\r\n",
        "KMeans is used to cluster embedded documents into groups where the TSNE model is used to visualize the clustering results.\r\n",
        "\r\n",
        "When the user does not specify the number of clusters for the documents, leaving `user_define_k` as `0` in the config file, The Elbow Method will be used to determine the optimal number of clusters.\r\n",
        "\r\n",
        "Confidence scores for each data point are calculated here as well using the Silhouette Coefficient and normalized to 0-1.\r\n",
        "\r\n",
        "\r\n",
        "<table><tr>\r\n",
        "<td>Example of Clustering</td>\r\n",
        "<td> <img width=60% src=\"../imgs/tsne_results.png\"/> </td>\r\n",
        "<td>Example of Elbow Method</td>\r\n",
        "<td> <img width=60% src=\"../imgs/silhouette.png\"/> </td>\r\n",
        "</tr></table>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not len(df_uncorrupted_not_blank) == 0:\r\n",
        "\r\n",
        "    st = time.time()\r\n",
        "    # Format embeddgings to dataframe\r\n",
        "    document_embeddings = np.expand_dims(np.vstack(pdf_embeddings+png_embeddings), axis=1).tolist()\r\n",
        "    document_embeddings_df = pd.DataFrame(document_embeddings, columns=[\"token0v2\"])\r\n",
        "    document_embeddings_df[\"corrupt\"] = corrupted_blank_pdf_list + blank_check_png_list\r\n",
        "    un_corrupted_embeddings = document_embeddings_df[document_embeddings_df[\"corrupt\"]==0].reset_index(drop=True)\r\n",
        "\r\n",
        "    # Kmeans with Elbow method\r\n",
        "    cluster_label_list, confidence_score_list, closest_to_centroid = cluster_kmeans_elbow(un_corrupted_embeddings, user_define_k = FIXK, max_tries = MAX_TRIES, show_tsne=True)\r\n",
        "    end = time.time()\r\n",
        "    logging.info(f\"Clustering, spent: {end-st}s\")\r\n",
        "\r\n",
        "\r\n",
        "    # update cluster ID and confidence score to final output\r\n",
        "    cluster_label_col = cluster_label_list + [\"no_content\"]*len(df_blank_documents) + [\"corrupt\"]*len(df_corrupted_documents)\r\n",
        "\r\n",
        "    confidence_score_col = confidence_score_list + [0]*(len(final_output)-len(cluster_label_list))\r\n",
        "    final_output[\"cluster_label\"] = cluster_label_col\r\n",
        "    final_output[\"confidence_score\"] = confidence_score_col\r\n",
        "    \r\n",
        "else:\r\n",
        "    final_output[\"cluster_label\"] = \"corrupt\"\r\n",
        "    final_output[\"confidence_score\"] = 0\r\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647364369761
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Description Generation\r\n",
        "\r\n",
        "KeyBERT is used to extract keywords from centroid documents. These keywords are then used as the descriptions of the clusters."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not len(df_uncorrupted_not_blank) == 0:\r\n",
        "    st = time.time()\r\n",
        "    # Load OCR\r\n",
        "    feature_extractor = LayoutLMv2FeatureExtractor()\r\n",
        "    tokenizer = LayoutLMv2Tokenizer.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\r\n",
        "    processor = LayoutLMv2Processor(feature_extractor, tokenizer)\r\n",
        "    \r\n",
        "    # Keyword generation\r\n",
        "    centroid_discription_paths = final_output.iloc[closest_to_centroid][\"pdf_path\"]\r\n",
        "    centroid_discriptions  = centroid_discription_paths.apply(pdf2keywords_keybert, \r\n",
        "                                                            processor=processor, \r\n",
        "                                                            keyphrase_ngram_range = literal_eval(KEYPHRASE_NGRAM_RANGE), \r\n",
        "                                                            top_n = TOP_N_KEYWORDS)\r\n",
        "    end = time.time()\r\n",
        "    logging.info(f\"Keyword extraction, spent: {end-st}s\")\r\n",
        "\r\n",
        "    # Add label discription to final output\r\n",
        "    final_output['label_description'] = final_output.apply(lambda x: assign_discriptions(x['cluster_label'],centroid_discriptions), axis=1)\r\n",
        "    final_output.drop(columns=['corrupt', 'no_content', 'pdf_path', 'corrupted_pdf'], axis = 1, inplace = True)\r\n",
        "\r\n",
        "else:\r\n",
        "    # If all doc(x)/rtf/png/pdf are corrupted or no doc(x)/rtf/png/pdf\r\n",
        "    # in the dataroom, then generate an empty dataframe output\r\n",
        "    final_output['label_description'] = \"corrupt\"\r\n",
        "    final_output.drop(columns=['corrupt', 'no_content', 'pdf_path', 'corrupted_pdf'], axis = 1, inplace = True)\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647364386258
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final JSON Output\r\n",
        "\r\n",
        "As the cluster labels, descriptions, and the confidence scores for PDF/DOC(X)/RTF files are obtained, these results are merged with the clustering results of the other document types.\r\n",
        "\r\n",
        "As shown in the workflow diagram at the beginning of the notebook, document types like .db, .xslx, zip,... will not go through the general process. They are grouped only by their file extensions, so the confidence score will be 1 for these file types."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge informaton of other data types into final output\r\n",
        "other_type_path_list = []\r\n",
        "\r\n",
        "for doc_type, doc_path_list in doc_type_dict.items():\r\n",
        "    if doc_type not in [\"pdf\",\"doc\",\"docx\",\"rtf\",\"png\"]:\r\n",
        "        for doc_path in doc_path_list:\r\n",
        "            other_type_path_list.append([doc_path, doc_type ,\"1\", doc_type])\r\n",
        "\r\n",
        "other_type_df = pd.DataFrame(other_type_path_list, columns=['document_path', 'cluster_label', 'confidence_score', 'label_description'])\r\n",
        "final_output = pd.concat([final_output, other_type_df], axis=0).reset_index(drop=True)\r\n",
        "\r\n",
        "final_output.to_json(OUTPUT_PATH, orient='records', lines=True)\r\n",
        "\r\n",
        "# Remove intermediate files\r\n",
        "if os.path.exists(DOC_TMP_FOLDER):\r\n",
        "    shutil.rmtree(DOC_TMP_FOLDER)\r\n",
        "\r\n",
        "if os.path.exists(TRANSFROMED_PDF_FOLDER):\r\n",
        "    shutil.rmtree(TRANSFROMED_PDF_FOLDER)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647364386568
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}